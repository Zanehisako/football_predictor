{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-12-19T16:16:23.670169Z",
     "iopub.status.busy": "2025-12-19T16:16:23.669752Z",
     "iopub.status.idle": "2025-12-19T16:16:23.689593Z",
     "shell.execute_reply": "2025-12-19T16:16:23.688694Z",
     "shell.execute_reply.started": "2025-12-19T16:16:23.670141Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "   ODDS DATA UNIFIER (MULTI-LEAGUE MERGE)         \n",
      "==================================================\n",
      "Processing odds_data/D1 (1).csv...\n",
      "Processing odds_data/D1.csv...\n",
      "Processing odds_data/E0 (1).csv...\n",
      "Processing odds_data/E0.csv...\n",
      "Processing odds_data/F1 (1).csv...\n",
      "Processing odds_data/F1.csv...\n",
      "Processing odds_data/I1 (1).csv...\n",
      "Processing odds_data/I1.csv...\n",
      "Processing odds_data/SP1 (1).csv...\n",
      "Processing odds_data/SP1.csv...\n",
      "\n",
      "âœ… SUCCESS: Created 'odds_data.csv'\n",
      "   Total Matches: 2597\n",
      "   Columns: Date, HomeTeam, AwayTeam, FTHG, FTAG, FTR, AvgH, AvgD, AvgA, Avg>2.5, Avg<2.5\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "\n",
    "print(\"==================================================\")\n",
    "print(\"   ODDS DATA UNIFIER (MULTI-LEAGUE MERGE)         \")\n",
    "print(\"==================================================\")\n",
    "\n",
    "# 1. Define the files you have\n",
    "# Put your filenames here\n",
    "files = [\n",
    "    \"odds_data/D1 (1).csv\", \"odds_data/D1.csv\",\n",
    "    \"odds_data/E0 (1).csv\", \"odds_data/E0.csv\",\n",
    "    \"odds_data/F1 (1).csv\", \"odds_data/F1.csv\",\n",
    "    \"odds_data/I1 (1).csv\", \"odds_data/I1.csv\",\n",
    "    \"odds_data/SP1 (1).csv\", \"odds_data/SP1.csv\"\n",
    "]\n",
    "\n",
    "# 2. Define the Columns we actually need\n",
    "# We prioritize 'Avg' (Market Average) but fallback to 'B365' (Bet365) if missing\n",
    "target_cols = ['Date', 'HomeTeam', 'AwayTeam', 'FTHG', 'FTAG', 'FTR']\n",
    "odds_priority = [\n",
    "    ('AvgH', 'B365H'), ('AvgD', 'B365D'), ('AvgA', 'B365A'), # 1X2\n",
    "    ('Avg>2.5', 'B365>2.5'), ('Avg<2.5', 'B365<2.5')         # Over/Under\n",
    "]\n",
    "\n",
    "master_list = []\n",
    "\n",
    "for f in files:\n",
    "    if not os.path.exists(f):\n",
    "        print(f\"âš ï¸ Warning: File '{f}' not found. Skipping.\")\n",
    "        continue\n",
    "        \n",
    "    print(f\"Processing {f}...\")\n",
    "    try:\n",
    "        # Load with robust parsing\n",
    "        df = pd.read_csv(f, encoding='unicode_escape', on_bad_lines='skip')\n",
    "        \n",
    "        # Standardize Columns\n",
    "        # Create a new mini-dataframe with standardized names\n",
    "        clean_df = df[target_cols].copy()\n",
    "        \n",
    "        # Smart Column Selection (Avg vs B365)\n",
    "        for avg_col, backup_col in odds_priority:\n",
    "            if avg_col in df.columns:\n",
    "                clean_df[avg_col] = df[avg_col] # Use Average if available\n",
    "            elif backup_col in df.columns:\n",
    "                clean_df[avg_col] = df[backup_col] # Use Bet365 as fallback\n",
    "            else:\n",
    "                clean_df[avg_col] = np.nan # Missing\n",
    "        \n",
    "        # Ensure date format is consistent\n",
    "        # Football-Data.co.uk usually uses dd/mm/yyyy or dd/mm/yy\n",
    "        clean_df['Date'] = pd.to_datetime(clean_df['Date'], dayfirst=True, errors='coerce')\n",
    "        \n",
    "        # Drop rows with no date or teams\n",
    "        clean_df = clean_df.dropna(subset=['Date', 'HomeTeam', 'AwayTeam'])\n",
    "        \n",
    "        master_list.append(clean_df)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error reading {f}: {e}\")\n",
    "\n",
    "# 3. Concatenate\n",
    "if master_list:\n",
    "    big_odds = pd.concat(master_list, ignore_index=True)\n",
    "    \n",
    "    # Save as the master file expected by your merge script\n",
    "    big_odds.to_csv(\"odds_data.csv\", index=False)\n",
    "    print(\"\\nâœ… SUCCESS: Created 'odds_data.csv'\")\n",
    "    print(f\"   Total Matches: {len(big_odds)}\")\n",
    "    print(\"   Columns: Date, HomeTeam, AwayTeam, FTHG, FTAG, FTR, AvgH, AvgD, AvgA, Avg>2.5, Avg<2.5\")\n",
    "else:\n",
    "    print(\"\\nâŒ No data found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "   ODDS INTEGRATION: SMART FUZZY MERGE            \n",
      "==================================================\n",
      "Stats: 2964 | Odds: 2597\n",
      "Running Smart Merge (Time tolerance +/- 1 day)...\n",
      "âœ… Matched 1947 games with odds.\n",
      "â„¹ï¸ 1017 games have NO odds (Training will handle this).\n",
      "ðŸ’¾ Saved to 'match_data_combined.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import difflib\n",
    "import re\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"==================================================\")\n",
    "print(\"   ODDS INTEGRATION ENGINE (FUZZY MATCHING)       \")\n",
    "print(\"==================================================\")\n",
    "\n",
    "# 1. LOAD DATA\n",
    "try:\n",
    "    df_stats = pd.read_csv(\"match_data.csv\")\n",
    "    # Robust load for odds (skip bad lines if files are messy)\n",
    "    df_odds = pd.read_csv(\"odds_data.csv\", encoding='unicode_escape', on_bad_lines='skip')\n",
    "    print(f\"âœ… Loaded Stats: {len(df_stats)} | Odds: {len(df_odds)}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"âŒ Error: Missing csv files.\")\n",
    "    exit()\n",
    "\n",
    "# 2. DATE PARSING (Crucial for merging)\n",
    "# FBref Date\n",
    "def parse_fbref_date(url):\n",
    "    try:\n",
    "        match = re.search(r'([A-Za-z]+-\\d{1,2}-\\d{4})', str(url))\n",
    "        if match: return pd.to_datetime(match.group(1), format='%B-%d-%Y')\n",
    "    except: pass\n",
    "    return pd.NaT\n",
    "\n",
    "df_stats['date_obj'] = df_stats['match_url'].apply(parse_fbref_date)\n",
    "\n",
    "# Odds Date (Handle multiple formats usually found in football-data.co.uk)\n",
    "df_odds['Date'] = pd.to_datetime(df_odds['Date'], dayfirst=True, errors='coerce')\n",
    "\n",
    "# 3. TEAM NAME MAPPING\n",
    "# We map Odds Names -> FBref Names\n",
    "fbref_teams = sorted(df_stats['home_team_name'].unique())\n",
    "odds_teams = sorted(df_odds['HomeTeam'].dropna().unique())\n",
    "\n",
    "team_map = {}\n",
    "\n",
    "# Manual overrides for known mismatches (Premier League examples)\n",
    "manual_map = {\n",
    "    \"Man United\": \"Manchester United\",\n",
    "    \"Man City\": \"Manchester City\",\n",
    "    \"Nott'm Forest\": \"Nottingham Forest\",\n",
    "    \"Spurs\": \"Tottenham Hotspur\",\n",
    "    \"Wolves\": \"Wolverhampton Wanderers\",\n",
    "    \"Brighton\": \"Brighton & Hove Albion\",\n",
    "    \"West Ham\": \"West Ham United\",\n",
    "    \"Leeds\": \"Leeds United\", \n",
    "    \"Leicester\": \"Leicester City\",\n",
    "    \"Newcastle\": \"Newcastle United\"\n",
    "}\n",
    "\n",
    "print(\"Mapping teams...\")\n",
    "for o_team in odds_teams:\n",
    "    if o_team in manual_map:\n",
    "        team_map[o_team] = manual_map[o_team]\n",
    "    else:\n",
    "        # Fuzzy Match\n",
    "        matches = difflib.get_close_matches(o_team, fbref_teams, n=1, cutoff=0.6)\n",
    "        if matches:\n",
    "            team_map[o_team] = matches[0]\n",
    "\n",
    "df_odds['mapped_home'] = df_odds['HomeTeam'].map(team_map)\n",
    "\n",
    "# 4. MERGE\n",
    "# We merge on 'Date' and 'Home Team'\n",
    "print(\"Merging data...\")\n",
    "merged = pd.merge(\n",
    "    df_stats, \n",
    "    df_odds[['Date', 'mapped_home', 'AvgH', 'AvgD', 'AvgA', 'B365H', 'B365D', 'B365A']], \n",
    "    left_on=['date_obj', 'home_team_name'], \n",
    "    right_on=['Date', 'mapped_home'], \n",
    "    how='left' # Keep all stats, fill missing odds with NaN\n",
    ")\n",
    "\n",
    "# 5. FEATURE ENGINEERING WITH ODDS\n",
    "# Prioritize Average Odds, fallback to Bet365\n",
    "merged['odds_home'] = merged['AvgH'].fillna(merged['B365H'])\n",
    "merged['odds_draw'] = merged['AvgD'].fillna(merged['B365D'])\n",
    "merged['odds_away'] = merged['AvgA'].fillna(merged['B365A'])\n",
    "\n",
    "# Convert to IMPLIED PROBABILITY (The Feature)\n",
    "# 1 / Odds = Probability\n",
    "merged['market_prob_home'] = 1 / merged['odds_home']\n",
    "merged['market_prob_draw'] = 1 / merged['odds_draw']\n",
    "merged['market_prob_away'] = 1 / merged['odds_away']\n",
    "\n",
    "# Fill Missing Odds (for matches not in odds database) with Neutral 33%\n",
    "# Add a flag so the model knows these are fake\n",
    "merged['has_odds'] = merged['odds_home'].notna().astype(int)\n",
    "merged[['market_prob_home', 'market_prob_draw', 'market_prob_away']] = merged[['market_prob_home', 'market_prob_draw', 'market_prob_away']].fillna(0.33)\n",
    "\n",
    "# Drop temp columns\n",
    "merged = merged.drop(columns=['Date', 'mapped_home', 'date_obj', 'AvgH', 'B365H'])\n",
    "\n",
    "print(f\"âœ… Final Dataset: {len(merged)} matches.\")\n",
    "print(f\"   Matches with valid odds found: {merged['has_odds'].sum()}\")\n",
    "\n",
    "merged.to_csv(\"match_data_with_odds.csv\", index=False)\n",
    "print(\"ðŸ’¾ Saved to 'match_data_with_odds.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8156144,
     "sourceId": 13661546,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4305,
     "sourceId": 13798781,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 3439120,
     "sourceId": 6004891,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31236,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
