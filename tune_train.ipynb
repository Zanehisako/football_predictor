{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f509eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "   HYPERPARAMETER TUNING ENGINE (ELO + ENSEMBLE)  \n",
      "==================================================\n",
      "Tuning on 1272 matches...\n",
      "\n",
      "--- Tuning XGBoost ---\n",
      "Fitting 5 folds for each of 144 candidates, totalling 720 fits\n",
      "✅ Best XGB Params: {'colsample_bytree': 0.7, 'gamma': 5, 'learning_rate': 0.03, 'max_depth': 4, 'n_estimators': 200, 'subsample': 0.7}\n",
      "✅ Best XGB Score: 53.21%\n",
      "\n",
      "--- Tuning Random Forest ---\n",
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n",
      "✅ Best RF Params: {'max_depth': 10, 'max_features': 'sqrt', 'min_samples_leaf': 3, 'n_estimators': 400}\n",
      "✅ Best RF Score: 48.40%\n",
      "\n",
      "--- Tuning Logistic Regression ---\n",
      "✅ Best LR Params: {'C': 0.01, 'solver': 'lbfgs'}\n",
      "✅ Best LR Score: 50.09%\n",
      "\n",
      "--- Training Final Ensemble with Calibration ---\n",
      "==================================================\n",
      "   CALIBRATED ACCURACY: 53.73%   \n",
      "==================================================\n",
      "Sample Probabilities (First 5 matches):\n",
      "[[0.13099245 0.16129816 0.70770939]\n",
      " [0.59450355 0.22137752 0.18411894]\n",
      " [0.52916373 0.2307189  0.24011737]\n",
      " [0.17435724 0.17113409 0.65450866]\n",
      " [0.17720345 0.23161336 0.5911832 ]]\n",
      "Threshold > 0.45: 189 matches | Accuracy: 59.26%\n",
      "Threshold > 0.50: 161 matches | Accuracy: 63.98%\n",
      "Threshold > 0.60: 95 matches | Accuracy: 69.47%\n",
      "Threshold > 0.70: 44 matches | Accuracy: 86.36%\n",
      "Threshold > 0.80: 16 matches | Accuracy: 81.25%\n",
      "✅ Calibrated Model saved.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import re\n",
    "import warnings\n",
    "import joblib\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"==================================================\")\n",
    "print(\"   HYPERPARAMETER TUNING ENGINE (ELO + ENSEMBLE)  \")\n",
    "print(\"==================================================\")\n",
    "\n",
    "# ==========================================\n",
    "# 1. LOAD & CLEAN (Same Feature Engineering)\n",
    "# ==========================================\n",
    "# We must recreate the exact features (ELO, Rolling, Rest) first\n",
    "df = pd.read_csv(\"match_data.csv\")\n",
    "\n",
    "def extract_date(url):\n",
    "    try:\n",
    "        match = re.search(r'([A-Za-z]+-\\d{1,2}-\\d{4})', str(url))\n",
    "        if match: return pd.to_datetime(match.group(1), format='%B-%d-%Y', errors='coerce')\n",
    "    except: pass\n",
    "    return pd.NaT\n",
    "\n",
    "df['date'] = df['match_url'].apply(extract_date)\n",
    "df = df.sort_values(by='date').reset_index(drop=True)\n",
    "\n",
    "# Basic Numeric Cleaning & Imputation\n",
    "stats_cols = [\"xg\", \"possession\", \"shots_onTarget\", \"corners\", \"fouls\", \"team_points\"]\n",
    "for side in ['home', 'away']:\n",
    "    p_col = f\"{side}_team_possession\"\n",
    "    if p_col in df.columns:\n",
    "        df[p_col] = pd.to_numeric(df[p_col].astype(str).str.rstrip('%'), errors='coerce').fillna(50) / 100.0\n",
    "    \n",
    "    # Calculate points if missing (3 for win logic)\n",
    "    if 'team_points' not in df.columns and f'{side}_team_score' in df.columns:\n",
    "        # (Logic handled in rolling block below)\n",
    "        pass\n",
    "\n",
    "    for s in stats_cols:\n",
    "        col = f\"{side}_{s}\" if s not in ['xg', 'possession', 'team_points'] else f\"{side}_team_{s}\"\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "            # Group median fill\n",
    "            df[col] = df[col].fillna(df.groupby(f'{side}_team_name')[col].transform('median'))\n",
    "            df[col] = df[col].fillna(df[col].mean()) # Fallback\n",
    "\n",
    "# --- ELO CALCULATION ---\n",
    "# --- ADVANCED ELO CALCULATION ---\n",
    "def calculate_elo(df):\n",
    "    k_factor = 25          # Slightly higher base K for responsiveness\n",
    "    home_advantage = 100    # Calibrated Home Advantage\n",
    "    \n",
    "    # Initialize ratings\n",
    "    team_elos = {team: 1500 for team in set(df['home_team_name']).union(set(df['away_team_name']))}\n",
    "    elo_h, elo_a = [], []\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        h, a = row['home_team_name'], row['away_team_name']\n",
    "        rh, ra = team_elos[h], team_elos[a]\n",
    "        elo_h.append(rh); elo_a.append(ra)\n",
    "        \n",
    "        # 1. Determine Result (S)\n",
    "        h_score, a_score = row['home_team_score'], row['away_team_score']\n",
    "        if h_score > a_score: S = 1\n",
    "        elif h_score == a_score: S = 0.5\n",
    "        else: S = 0\n",
    "        \n",
    "        # 2. Calculate Expected Result (E)\n",
    "        dr = (rh + home_advantage) - ra\n",
    "        e_h = 1 / (1 + 10 ** (-dr / 400))\n",
    "        \n",
    "        # 3. Margin of Victory Multiplier (M)\n",
    "        # Formula: ln(abs(GoalDiff) + 1) * (2.2 / ((EloDiff * 0.001) + 2.2))\n",
    "        # This gives extra credit for blowouts, but scales down if the ELO gap was huge anyway.\n",
    "        goal_diff = abs(h_score - a_score)\n",
    "        if goal_diff == 0:\n",
    "            M = 1 # No multiplier for draws\n",
    "        else:\n",
    "            # Elo difference bias: If strong team wins big, multiplier is smaller than if weak team wins big\n",
    "            elo_diff_bias = 2.2 / ((0.001 * dr if S == 1 else -0.001 * dr) + 2.2)\n",
    "            M = np.log(goal_diff + 1) * elo_diff_bias\n",
    "            \n",
    "        # 4. Update Ratings\n",
    "        # New Rating = Old Rating + (K * M * (Actual - Expected))\n",
    "        point_change = k_factor * M * (S - e_h)\n",
    "        \n",
    "        team_elos[h] = rh + point_change\n",
    "        team_elos[a] = ra - point_change # Zero-sum update\n",
    "        \n",
    "    return elo_h, elo_a\n",
    "\n",
    "df['home_elo'], df['away_elo'] = calculate_elo(df)\n",
    "df['diff_elo'] = (df['home_elo'] + 65) - df['away_elo']\n",
    "\n",
    "# --- REST DAYS ---\n",
    "def calc_rest(df):\n",
    "    long_df = pd.concat([\n",
    "        df[['date', 'home_team_name']].rename(columns={'home_team_name':'team'}),\n",
    "        df[['date', 'away_team_name']].rename(columns={'away_team_name':'team'})\n",
    "    ]).sort_values(['team', 'date'])\n",
    "    long_df['rest'] = (long_df['date'] - long_df.groupby('team')['date'].shift(1)).dt.days.fillna(7).clip(upper=14)\n",
    "    return dict(zip(zip(long_df['date'], long_df['team']), long_df['rest']))\n",
    "\n",
    "rest_map = calc_rest(df)\n",
    "df['diff_rest'] = df.apply(lambda x: rest_map.get((x['date'], x['home_team_name']),7), axis=1) - \\\n",
    "                  df.apply(lambda x: rest_map.get((x['date'], x['away_team_name']),7), axis=1)\n",
    "\n",
    "# --- ROLLING STATS ---\n",
    "# Recalculate points specifically for rolling\n",
    "df['home_team_points'] = np.select([df['home_team_score']>df['away_team_score'], df['home_team_score']==df['away_team_score']], [3, 1], 0)\n",
    "df['away_team_points'] = np.select([df['away_team_score']>df['home_team_score'], df['away_team_score']==df['home_team_score']], [3, 1], 0)\n",
    "\n",
    "roll_feats = ['team_xg', 'team_possession', 'shots_onTarget', 'corners', 'team_points',\"fouls\"]\n",
    "h_d = df[['date', 'match_url', 'home_team_name']].rename(columns={'home_team_name':'team'})\n",
    "a_d = df[['date', 'match_url', 'away_team_name']].rename(columns={'away_team_name':'team'})\n",
    "\n",
    "for f in roll_feats:\n",
    "    c_h = f\"home_{f}\" if f in ['team_points', 'team_xg', 'team_possession'] else f\"home_{f}\"\n",
    "    c_a = f\"away_{f}\" if f in ['team_points', 'team_xg', 'team_possession'] else f\"away_{f}\"\n",
    "    if c_h in df.columns: h_d[f] = df[c_h]\n",
    "    if c_a in df.columns: a_d[f] = df[c_a]\n",
    "\n",
    "stacked = pd.concat([h_d, a_d]).sort_values(['team', 'date'])\n",
    "for f in roll_feats:\n",
    "    if f in stacked.columns:\n",
    "        stacked[f'roll_{f}'] = stacked.groupby('team')[f].transform(lambda x: x.shift(1).rolling(5, min_periods=1).mean())\n",
    "\n",
    "df = df.merge(stacked[['match_url', 'team'] + [f'roll_{f}' for f in roll_feats]], left_on=['match_url', 'home_team_name'], right_on=['match_url', 'team'], how='left').drop(columns=['team']).rename(columns={f'roll_{f}': f'home_roll_{f}' for f in roll_feats})\n",
    "df = df.merge(stacked[['match_url', 'team'] + [f'roll_{f}' for f in roll_feats]], left_on=['match_url', 'away_team_name'], right_on=['match_url', 'team'], how='left').drop(columns=['team']).rename(columns={f'roll_{f}': f'away_roll_{f}' for f in roll_feats})\n",
    "\n",
    "for f in roll_feats:\n",
    "    df[f'diff_{f}'] = df[f'home_roll_{f}'] - df[f'away_roll_{f}']\n",
    "\n",
    "df = df.fillna(0)\n",
    "\n",
    "# ==========================================\n",
    "# 2. PREPARE FOR TUNING\n",
    "# ==========================================\n",
    "conditions = [\n",
    "    (df['home_team_score'] > df['away_team_score']),\n",
    "    (df['home_team_score'] == df['away_team_score']),\n",
    "    (df['home_team_score'] < df['away_team_score'])\n",
    "]\n",
    "df['match_outcome'] = np.select(conditions, [2, 1, 0])\n",
    "\n",
    "features = [\n",
    "    'diff_elo', 'home_elo', 'away_elo',\n",
    "    'diff_rest', 'diff_team_points', 'diff_team_xg', \n",
    "    'diff_shots_onTarget', 'diff_corners',\n",
    "    'home_roll_team_xg', 'away_roll_team_xg',\n",
    "    'home_roll_team_possession', 'away_roll_team_possession',\n",
    "    'home_roll_shots_onTarget', 'away_roll_shots_onTarget',\n",
    "    'home_roll_corners', 'away_roll_corners',\n",
    "    'home_roll_fouls', 'away_roll_fouls',\n",
    "]\n",
    "\n",
    "X = df[features]\n",
    "y = df['match_outcome']\n",
    "split = int(len(df) * 0.8)\n",
    "\n",
    "X_train, X_test, y_train, y_test = X[:split], X[split:], y[:split], y[split:]\n",
    "\n",
    "# Time Series Split (Crucial for validity)\n",
    "# 4 splits = Test on last 20%, then last 40%, etc.\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "print(f\"Tuning on {len(X)} matches...\")\n",
    "\n",
    "# ==========================================\n",
    "# 3. TUNE XGBOOST\n",
    "# ==========================================\n",
    "print(\"\\n--- Tuning XGBoost ---\")\n",
    "xgb_param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'learning_rate': [0.01, 0.03, 0.05],\n",
    "    'max_depth': [3, 4],\n",
    "    'subsample': [0.7, 0.8],\n",
    "    'colsample_bytree': [0.7, 0.8],\n",
    "    'gamma': [1, 5] # Regularization\n",
    "}\n",
    "\n",
    "xgb_model = xgb.XGBClassifier(objective='multi:softprob', num_class=3, tree_method='hist', random_state=42)\n",
    "xgb_search = GridSearchCV(xgb_model, xgb_param_grid, cv=tscv, scoring='accuracy', n_jobs=-1, verbose=1)\n",
    "xgb_search.fit(X, y)\n",
    "\n",
    "print(f\"✅ Best XGB Params: {xgb_search.best_params_}\")\n",
    "print(f\"✅ Best XGB Score: {xgb_search.best_score_:.2%}\")\n",
    "best_xgb = xgb_search.best_estimator_\n",
    "\n",
    "# ==========================================\n",
    "# 4. TUNE RANDOM FOREST\n",
    "# ==========================================\n",
    "print(\"\\n--- Tuning Random Forest ---\")\n",
    "rf_param_grid = {\n",
    "    'n_estimators': [200, 400],\n",
    "    'max_depth': [5, 8, 10], # Keep shallow to prevent overfitting noise\n",
    "    'min_samples_leaf': [3, 5, 10], # Higher = more generalized\n",
    "    'max_features': ['sqrt', 0.5]\n",
    "}\n",
    "\n",
    "rf_model = RandomForestClassifier(random_state=42,class_weight='balanced')\n",
    "rf_search = GridSearchCV(rf_model, rf_param_grid, cv=tscv, scoring='accuracy', n_jobs=-1, verbose=1)\n",
    "rf_search.fit(X, y)\n",
    "\n",
    "print(f\"✅ Best RF Params: {rf_search.best_params_}\")\n",
    "print(f\"✅ Best RF Score: {rf_search.best_score_:.2%}\")\n",
    "best_rf = rf_search.best_estimator_\n",
    "\n",
    "# ==========================================\n",
    "# 5. TUNE LOGISTIC REGRESSION\n",
    "# ==========================================\n",
    "print(\"\\n--- Tuning Logistic Regression ---\")\n",
    "lr_param_grid = {\n",
    "    'C': [0.01, 0.1, 1.0, 10.0], # Regularization strength\n",
    "    'solver': ['lbfgs', 'newton-cg']\n",
    "}\n",
    "\n",
    "scaler = StandardScaler() # LR needs scaled data\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "lr_model = LogisticRegression(multi_class='multinomial', max_iter=2000,class_weight='balanced')\n",
    "lr_search = GridSearchCV(lr_model, lr_param_grid, cv=tscv, scoring='accuracy', n_jobs=-1)\n",
    "lr_search.fit(X_scaled, y)\n",
    "\n",
    "print(f\"✅ Best LR Params: {lr_search.best_params_}\")\n",
    "print(f\"✅ Best LR Score: {lr_search.best_score_:.2%}\")\n",
    "best_lr = lr_search.best_estimator_\n",
    "\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "# ==========================================\n",
    "# 6. BUILD FINAL CALIBRATED ENSEMBLE\n",
    "# ==========================================\n",
    "print(\"\\n--- Training Final Ensemble with Calibration ---\")\n",
    "\n",
    "# 1. Define the Base Ensemble\n",
    "# We stick to the weights that gave you 52%+ accuracy\n",
    "base_ensemble = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('xgb', best_xgb),\n",
    "        ('rf', best_rf),\n",
    "        ('lr', best_lr)\n",
    "    ],\n",
    "    voting='soft',\n",
    "    weights=[2, 1, 5] \n",
    ")\n",
    "\n",
    "# 2. Calibrate the Ensemble\n",
    "# \"Isotonic\" calibration fixes the under-confidence by mapping model outputs to real win rates.\n",
    "# cv='prefit' means we use the data we already trained on (or we split X_train again).\n",
    "# Since we want to use all X_train, we use 3-fold CV calibration.\n",
    "calibrated_ensemble = CalibratedClassifierCV(\n",
    "    estimator=base_ensemble,\n",
    "    method='isotonic', # 'isotonic' is best for fixing under-confidence\n",
    "    cv=3\n",
    ")\n",
    "\n",
    "calibrated_ensemble.fit(X_train, y_train)\n",
    "\n",
    "# ==========================================\n",
    "# 7. FINAL EVALUATION\n",
    "# ==========================================\n",
    "preds = calibrated_ensemble.predict(X_test)\n",
    "probs = calibrated_ensemble.predict_proba(X_test)\n",
    "acc = accuracy_score(y_test, preds)\n",
    "\n",
    "print(\"==================================================\")\n",
    "print(f\"   CALIBRATED ACCURACY: {acc:.2%}   \")\n",
    "print(\"==================================================\")\n",
    "\n",
    "# Check the new Probabilities\n",
    "print(\"Sample Probabilities (First 5 matches):\")\n",
    "print(probs[:5])\n",
    "\n",
    "# Value Bets Check\n",
    "results = pd.DataFrame({\n",
    "    'Home': df.iloc[split:]['home_team_name'],\n",
    "    'Away': df.iloc[split:]['away_team_name'],\n",
    "    'Actual': y_test.values,\n",
    "    'Pred': preds,\n",
    "    'Conf': np.max(probs, axis=1)\n",
    "})\n",
    "\n",
    "for t in [0.45, 0.50, 0.60, 0.70, 0.80]:\n",
    "    sub = results[results['Conf'] > t]\n",
    "    if len(sub) > 0:\n",
    "        print(f\"Threshold > {t:.2f}: {len(sub)} matches | Accuracy: {accuracy_score(sub['Actual'], sub['Pred']):.2%}\")\n",
    "\n",
    "# Save the CALIBRATED model\n",
    "joblib.dump({\n",
    "    'model': calibrated_ensemble, \n",
    "    'features': features,\n",
    "    'elo_dict': dict(zip(df['home_team_name'], df['home_elo'])),\n",
    "    'df_recent': df[['date', 'home_team_name', 'away_team_name'] + [c for c in df.columns if 'roll_' in c]].tail(1000)\n",
    "}, 'football_model_final.pkl')\n",
    "\n",
    "print(\"✅ Calibrated Model saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3c038fc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[49  0 35]\n",
      " [13  0 34]\n",
      " [36  0 88]]\n",
      "Accuracy: 53.73%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(y_test, preds)\n",
    "print(cm)\n",
    "print(f\"Accuracy: {accuracy_score(y_test, preds):.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4272f50e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
